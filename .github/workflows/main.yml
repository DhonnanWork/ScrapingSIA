# A friendly name for the workflow, which will be displayed in the GitHub Actions tab.
name: Run SIA Scraper

# --- Triggers ---
# Defines when the workflow will run.
on:
  # Allows you to manually trigger this workflow from the GitHub UI.
  workflow_dispatch:

  # Runs the workflow on a schedule.
  # The cron string '0 */6 * * *' means "at minute 0 past every 6th hour".
  # This will run the scraper 4 times a day (at 00:00, 06:00, 12:00, 18:00 UTC).
  schedule:
    - cron: '0 */6 * * *'

# --- Jobs ---
# A workflow run is made up of one or more jobs, which run in parallel by default.
jobs:
  # The single job in this workflow, named 'scrape'.
  scrape:
    # The type of virtual machine to run the job on. 'ubuntu-latest' is the standard choice.
    runs-on: ubuntu-latest

    # A sequence of tasks called 'steps' that will be executed as part of the job.
    steps:
      # Step 1: Check out your repository code
      # This action checks-out your repository under $GITHUB_WORKSPACE, so your job can access it.
      - name: Checkout Repository
        uses: actions/checkout@v4

      # Step 2: Set up the Python environment
      # This action installs a specific version of Python and adds it to the PATH.
      - name: Set up Python 3.12
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'

      # Step 3: Cache Pip dependencies
      # This speeds up the build by restoring downloaded packages from a previous run.
      - name: Cache pip
        uses: actions/cache@v4
        with:
          # The path to the directory that should be cached. Pip's cache is here on Linux.
          path: ~/.cache/pip
          # An explicit key for restoring and saving the cache.
          # It uses the requirements.txt hash, so if dependencies change, a new cache is created.
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
          # A list of keys to use for restoring the cache if the primary key doesn't match.
          restore-keys: |
            ${{ runner.os }}-pip-

      # Step 4: Cache Playwright browser binaries
      # Playwright browsers are large; caching them saves a lot of time.
      - name: Cache Playwright browsers
        uses: actions/cache@v4
        with:
          # The path where Playwright stores its browser binaries on Linux.
          path: ~/.cache/ms-playwright
          # A unique key for the browser cache.
          key: ${{ runner.os }}-playwright-${{ hashFiles('**/requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-playwright-

      # Step 5: Install dependencies and browser
      # This step runs command-line programs.
      - name: Install Dependencies and Browser
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          # The '--with-deps' flag installs necessary OS libraries for Firefox to run headlessly.
          python -m playwright install --with-deps firefox

      # Step 6: Run the main scraper script
      - name: Run Scraper.py
        # Securely pass secrets from your repository settings to the script as environment variables.
        env:
          NIM: ${{ secrets.NIM }}
          PASSWORD: ${{ secrets.PASSWORD }}
          GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
        run: |
          # Make sure the scraper runs in headless mode in the script itself.
          python Scraper.py

      # Step 7: Upload scraped data and logs as an artifact
      # 'if: always()' ensures this step runs even if the 'Run Scraper.py' step fails.
      # This is crucial for debugging, as it saves the logs and error screenshots.
      - name: Upload Scraped Data and Logs
        if: always()
        uses: actions/upload-artifact@v4
        with:
          # The name of the artifact to be uploaded.
          name: scraped-output
          # A list of files and directories to include in the artifact.
          # Wildcards can be used. This will grab all .json files, logs, and any error screenshots.
          path: |
            scraped_data/
            scraper_crash.log
            critical_error_page.png
            login_failure_final_page.png